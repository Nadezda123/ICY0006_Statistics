---
title: "Medical Cost Personal Datasets"
output: html_notebook
---

## STAGE 1
#### Step 1

Read your dataset. Done

#### Step 2

Write a description of your dataset:
• What is the topic?
• Where you get it from? 
• What kind of variables you have (numerical/categorical etc)? Describe the variables (the meaning of those).


***Medical Cost Personal Datasets***

The dateset was used from https://www.kaggle.com/mirichoi0218/insurance

Short Descriptions of variables:

The independent variable is manipulated by the experimenter and its effects on the dependent variable are measured.

Qualitative variables (categorical variables) are those that express a qualitative attribute such as hair colour, eye colour, religion, favourite movie, gender, and so on.

Quantitative variables are those variables that are measured in terms of numbers (height, weight, shoe size).

Discrete variables can take only certain values (For example, a household could have three children or six children, but not 4.53 children)

Continuous variables can take any value within the range of the scale (For example, "time to respond to a question" are continuous variables since the scale is continuous,say, the response time could be 1.64 seconds).


The dataset consist of 1338 observations and 7 variables.

AGE - independent, qualitative and discrete

SEX - categorical, qualitative

BMI - independent continuous variable, quantitative

Nr of CHILDREN - independent discrete variable, qualitative

SMOKER - independent variable, categorical

REGION - independent variable, categorical

CHARGES -dependent continuous variable, quantitative


#### Step 3

Look for missing values, or errors (NA etc) in the dataset. 


```{r}
which(is.na(insurance))
```

The return is 0, means Dataset is complete.

#### Step 4
#### Step 5

Do some data visualization (distribution graphs).
Write short report

```{r}
#summary of our dataset, where we can see sample of the data and variable data type, as integer, character, number.
str(insurance)
```
Now we will visualize each variable separately. For that we can use the Density plots. 

```{r}
#visualizationof Age variable
insurance <- read.csv("insurance.csv", header = TRUE, sep = ",")
ggplot(insurance, aes(age)) +
geom_histogram(aes(y=..density..), alpha = 0.7, col = "red") +
geom_density(col = "blue") +
labs(title = "Density of Age") +
theme(plot.title = element_text(hjust = 0.5))
```

Now we can visualize charges distribution based on Age, to see the correlation between the variables.

```{r}
# medical charges for the Age variable
ggplot(insurance, aes(x = as.factor(age), y = charges, fill = as.factor(age))) +
geom_boxplot() +
labs(title = "Medical Costs based on Age",
x = "Age") +
theme(plot.title = element_text(hjust = 0.5))

```
Based on this plot we can see that age is very strong predicator in our set, but we will have to test other variables to see a full picture. Also based on this plot we can see that there are in total 3 layers exist (dots below 20k, on the 20k line and on the 
40k), so it means that there are 2 more variables that have valuable correlation  with charges.



```{r}
# Visualizing of Gender
ggplot(insurance, aes(x = sex, fill = sex)) +
geom_bar(aes(stat = "identity"), alpha = 0.7) +
labs(title = "Count of Gender") +
theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#Gender Count Table
table(insurance$sex)
```
Now we can visualize charges distributions vs gender.

```{r}
ggplot(insurance, aes(x = sex, y = charges, color = sex)) +
geom_boxplot() +
labs(title = "Medical Costs by Gender") +
theme(plot.title = element_text(hjust = 0.5))
```
Based on this plot we can see that there no big difference in charges distributions based on Gender.

```{r}
# Density table of BMI
ggplot(insurance, aes(bmi)) +
geom_histogram(aes(y=..density..), alpha = 0.7, col = "red") +
geom_density(col = "blue") +
labs(title = "Density of BMI") +
theme(plot.title = element_text(hjust = 0.5))
```

BMI histogram indicates the normal distribution with single symmetrical pick.
Now it will be interesting to see if BMI will have an effect on our Medical charges. Based on the following website
https://www.cancer.org/cancer/cancer-causes/diet-physical-activity/body-weight-and-cancer-risk/adult-bmi.html
we have an official data on BMI description (This data will help us to analize our table):
Underweight: BMI is less than 18.5
Normal weight: BMI is 18.5 to 24.9
Overweight: BMI is 25 to 29.9
Obese: BMI is 30 or more


```{r}
#plot charges and bmi
ggplot(insurance, aes(x = bmi, y = charges, colour = "pink")) +
geom_point() +
labs(title = "Medical Costs vs BMI") +
theme(plot.title = element_text(hjust = 0.5))
          
```
Based on this plot we can see that charges are growing with BMI increasing.
The highest charges we have where BMI over 30 and based on our data above it means that these people are obese. Please note that it is not the main factor, but definitely plays the role.




```{r}
#Children Table
table(insurance$children)
```
```{r}
#Density of Children
ggplot(insurance, aes(children)) +
geom_histogram(aes(y=..density..), alpha = 0.7, col = "red") +
geom_density(col = "blue") +
labs(title = "Density of Children") +
theme(plot.title = element_text(hjust = 0.5))
```



Based on the Table we can see that 574 people doesn't have any children and 18 people have the maximum of 5 children in this Dataset. Now we will try to see if children have an effect on the Medical Charges. Logically we would think that the more children someone has the higher the medical charges, interesting will be to test this assumption (based at least on this dataset)


```{r}
# medical costs for Number of Children
ggplot(insurance, aes(x = as.factor(children), y = charges, color = as.factor(children))) +
geom_boxplot() +
labs(title = "Medical Costs By Number Of Children",
x = "Number of Children") +
theme(plot.title = element_text(hjust = 0.5),
legend.position = "none")
```

Based on this plot we can see that our assumption is not correct and number of children might not be the valuable factor in the increasing the Medical Charges (based on our data). In other scenarios where sample groups for people with 5 children will be big enough we might see different result.


```{r}
#Count table of Smoker
table(insurance$smoker)
```


```{r}
# Visualizing Smoker
ggplot(insurance, aes(x = smoker, fill = smoker)) +
geom_bar(aes(stat = "identity"), alpha = 0.7) +
labs(title = "Count of Smoker") +
theme(plot.title = element_text(hjust = 0.5))
```

Now we can try to see if person smokes how it will affect our distribution cost.

```{r}
#distribution of charges based on Smoking or not
ggplot(insurance, aes(x = smoker, y = charges, fill = smoker)) +
geom_boxplot()+
labs(title = "Medical Charges Distribution based on Smoking") +
theme(plot.title = element_text(hjust = 0.8))
```
Based on our plot we can see that if person smokes it has very significant factor for increasing the Medical charges. 



```{r}
#Table to Count Regions
table(insurance$region)
```
```{r}
# visualizing Regions
ggplot(insurance, aes(x = region, fill = region)) +
geom_bar(aes(stat = "identity"), alpha = 0.7) +
labs(title = "Count of Regions") +
theme(plot.title = element_text(hjust = 0.5))
```
Now we can try to see if region is important predictor in our model or not.

```{r}
ggplot(insurance, aes(x = region, y = charges, color = region)) +
geom_boxplot() +
labs(title = "Medical Costs based on Region") +
theme(plot.title = element_text(hjust = 0.5))
```
Based on our plot there are no significant differences based on the charges for the particular regions in our dataset.



```{r}
#visualizing Charges
ggplot(insurance, aes(charges)) +
geom_histogram(aes(y=..density..), alpha = 0.7, col = "red") +
geom_density(col = "blue") +
labs(title = "Density of Medical Charges") +
theme(plot.title = element_text(hjust = 0.5))
```
On this distribution we can see that it is right skewed and has two peaks. The first peak is around 2-3k and the second, much smaller pick, is around 40k. Charges histogram is a right-skewed distribution or so called positively skewed distribution. Large portion of data values occur on the left side with a fewer number of data values on the right side.




#### Step 5
Summary
Age, smoking and bmi factors have the highest impact on the medical charges.
Gender, number of children and region have the lowest impact on the medical charges.



## STAGE 2
#### Step 1

Do some quantitive overview

- compute central tendency measures (mean, mode, median, etc)

Central Tendency- is a typical or central value for a probability distribution. It is a descriptive summary of a dataset through a single value that reflects the center of the data distribution. Although it does not provide information regarding the individual values in the dataset, it delivers a comprehensive summary of the whole dataset.

Mean

It is the sum of all values divided by the total number of values.
It shows the balance point in the distribution. For discrete variables mean values we can convert to  integer.

Median

It is the middle number in an ordered data set.
```{r}
#central tendency measures for the Age
summary(insurance$age)
```
```{r}
#central tendency measures for the bmi
summary(insurance$bmi)
```
```{r}
#central tendency measures for the children variable
summary(insurance$children)
```
```{r}
#central tendency measures for the charges
summary(insurance$charges)
```


Mode
The mode is the value that has highest number of occurrences in a set of data. Unlike mean and median, mode can have both numeric and character data (the most frequent value.).

```{r}
# Create the function.
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

getmode(insurance$region)
getmode(insurance$sex)
getmode(insurance$smoker)
getmode(insurance$age)
getmode(insurance$bmi)
getmode(insurance$children)
getmode(insurance$charges)
```

```{r}
#Summary of Central Tendency Measures
summary(insurance)
```


Trimean

It is an estimate of central tendency, and is calculated as a weighted average of the median and the two quartiles of a set of values (weighted average of 25th, 50th and 75th percentils)



- compute variability measures (Range, IQR, Standard Deviation, Variance)

*** Range
The diffrence between the highest and lowest values
```{r}
range(insurance$age)
```
```{r}
range(insurance$bmi)
```
```{r}
range(insurance$children)
```
```{r}
range(insurance$charges)
```


Interquartile Range

The interquartile range (IQR) contains the second and third quartiles, or the middle half of the data set. Whereas the range gives the spread of the whole data set, the interquartile range gives the range of the middle half of a data set.

```{r}
IQR(insurance$age)
```
```{r}
IQR(insurance$bmi)
```

```{r}
IQR(insurance$children)
```
```{r}
IQR(insurance$charges)
```


Standard Deviation

Average distance from the mean. A high standard deviation means that values are generally far from the mean, while a low standard deviation indicates that values are clustered close to the mean.
Standard deviation is a useful measure of spread for normal distributions.
```{r}
head(insurance)
```


```{r}
# to compute Standard deviation we can use the following formula (we compute the square root of our variance):
  sqrt(var(insurance))
```


Variance

Average of squared distances from the mean.
```{r}
#for age, bmi, children, charges
var(insurance)
```




#### Step 2

Write a summary
 
 - What can you use from those central tendency/ variability measures?
 - Does it corresponds to visual picture of distribution?
 - What you can conclude from this analysis?
 
 While a measure of central tendency describes the typical value, measures of variability define how far away the data points tend to fall from the center. The variability in the context of a distribution of values.
The conclusion is that data sets could have the same central tendency, but different levels of variability or vice versa. Together they give us complete picture of our data.
The variability measures corresponds nicely with our visualizations of the distribution and give exact values that sometimes are hard to see from the visual representation.
 
 
 
## Stage 3
 
#### Step 1
 
 Try to find out, if there are some linear relationships between variables:
 
- Pick up only those variable, which are suitable. Remove outliers if required.
- Compute correlation matrix. It would be nice if you can express it visually.


To find out the linear relationship between the variables, we will choose suitable variables like age, bmi, children and charges. First we have to remove outliers.
Outliers are data points that fall far away from the major “cluster” of points. They can be legit data points carrying valuable information or can be erroneous values altogether. 
There are multiple ways on how to detect potential Outliers. The most notable ones are The Inter Quartile Range method and the z-score method. We will use IQR method to detect outliers and boxplot to visualize the outliers.

```{r}
#we can use visualization to check for outliers by using boxplot

par(mfrow = c(1,3))
par("mar")
par(mar=c(1,1,1,1))
boxplot(insurance$age, main = "Boxplot of Age")
boxplot(insurance$bmi, main = "Boxplot of BMI")
boxplot(insurance$children, main = "Boxplot of Children")

```


Based on the boxplots above we can see that we have few outliers in bmi boxplot
We can use quantile function, IQR function and subset function to find exact value and remove outliers

```{r}
# removing outliers with IQR
# find Q1, Q3 and IQR for values in column Age
Q1 <-quantile(insurance$bmi, .25)
Q3 <- quantile(insurance$bmi, .75)
IQR <- IQR(insurance$bmi)
# only keep rows that have values within 1.5*IQR of Q1 and Q3
insurance_2 <- subset(insurance, insurance$bmi> (Q1 - 1.5*IQR) & insurance$bmi< (Q3 + 1.5*IQR))
#we can compare to the original value that was 1338
str(insurance_2)
```
```{r}
#we can visualize that outliers removed
par(mfrow = c(1,3))
par("mar")
par(mar=c(1,1,1,1))
boxplot(insurance_2$bmi, main = "Boxplot of BMI")
```
We can see that outliers were removed from our bmi boxplot and now we have 1329 observations instead of 1338.



```{r}
#input <- insurance_2[,c('bmi','charges')]
#print(head(input))
#plot(x = input$bmi, y = input$charges,
#     xlab = "BMI",
#     ylab = "Charges",
#     xlim = c(1, 70),
#     ylim = c(1000,60000),
#     main = "BMI vs Charges"
#)
```


***Correlation matrix***

Correlation matrix showing the relationship between two or more variables and the interrelation in their movements.
The matrix depicts the correlation between all the possible pairs of values in a table. It consists of rows and columns that show the variables. Each cell in a table contains the correlation coefficient.
The correlation coefficient's values range between -1.0 and 1.0.
Based on Pearson's correlation value table we can better understand the relationship between the variables:

0- No correlation
-0.2 to 0 /0 to 0.2 – very weak negative/ positive correlation
-0.4 to -0.2/0.2 to 0.4 – weak negative/positive correlation
-0.6 to -0.4/0.4 to 0.6 – moderate negative/positive correlation
-0.8 to -0.6/0.6 to 0.8 – strong negative/positive correlation
-1 to -0.8/0.8 to 1 – very strong negative/positive correlation
-1/1 – perfectly negative/positive correlation


```{r}
#correlation values
library("corrplot")
cor(insurance_2[sapply(insurance_2, is.numeric)])

```
Based on our values (rounded values):
- First cell is always be 1 because it represents the relationship between the same variable (we are not interested in that kind of relationship).
BMI/age - 0.11 - 
Children/age - 0.04
Charges/age - 0.3
bmi/children - 0.02
bmi/charges - 0.19
children/charges - 0.07
Based on our values we have the highest positive correlation between age and charges, which is 0.3. The other relationships that note to mention are bmi/charges, which is approx 0.2. And the slight relationship we can see between the bmi and age, which is 0.1.


```{r}
#we can create Correlation matrix visualization to better see the relationships

library(GGally)
library("corrplot")
mat_1 <- as.matrix(cor((insurance_2[sapply(insurance_2, is.numeric)])))
mat_1
corrplot(cor(mat_1), title = "Correlations between numeric variables", order="hclust", tl.col="black")
```
In this plot the size and shade of each circle represents the strength of each relationship, while the color represents the direction, either negative or positive.
We can see that the highest positive correlation, is between Age and Charges (matching our correlation value table) In correlation table 0.3 -is called weak positive correlation, but it is the highest from what we have.



#### Step 2
Pay attention to dependent variable:

- From correlation analysis you know which variables have high correlation with your dependent variable.
- Draw the scatter plots "Dependent variable vs independent variable"
- Write down your observations
Based on our correlation analyses and scatter plot, we can conclude that these two variables share a common covariance and are homoscedastic.


From the our correlation matrix we can see the charges variable is our dependent variable, so we will create the scatterplots for charges vs age/bmi/children (not categorical variables)

But first I would like to make the scatterplot for all the variables (except charges) to view the relationship between the categorical variables as well.
```{r}
#we can also create another version of complete Scatter plot matrix between all the variables in our dataset (excluding charges )
library(psych)
pairs.panels(insurance_2[c("age", "sex", "bmi", "children", "smoker", "region")], digits = 2, cor = TRUE, main = "Insurance Matrix")
```
In the scatterplot matrix:
- red dot shows the mean
- oval means more correlation and if circle is more round then less correlation.
Based on the plot above, we can see that there are no strong correlations between these variables, r < 0.2



We will proceed now to create our scatterplots for the highest correlations, charges vs age/bmi/children
```{r}
#Age vs charges
input <- insurance_2[,c('age','charges')]

plot(x = input$age, y = input$charges,
     xlab = "Age",
     ylab = "Charges",
     xlim = c(15, 70),
     ylim = c(1000,60000),
     main = "Dependent vs independent variable"
)
```

Based on the above scatterplot we can see that charges are in 3 ranges (we saw it already in Stage 1) and steadily increasing with age.
We also see:
- positive relationship between variables,
- smooth increasing slope of the lines,
- how tightly the points cluster, specially at the bottom range (this means the linear relationship)
- homoscedasticity
Meaning there is a pretty moderate relationship between the age and the charges: the older the person the higher the charges.But there some factor, probably due to smoker or not that affecting and creating these ranges




```{r}
#BMI vs charges
input <- insurance_2[,c('bmi','charges')]

plot(x = input$bmi, y = input$charges,
     xlab = "BMI",
     ylab = "Charges",
     xlim = c(10, 50),
     ylim = c(1000,60000),
     main = "Dependent vs independent variable"
)
```

We see very weak negative relationship between the charges and bmi, where mostly the points cluster along the below line. There is also minimum homoscedasticity in the plot.
Based on the above we can see that charges are mostly below 15k and only few are above 40k. And there other factors that influence the charges.


```{r}
# Children vs charges
input <- insurance_2[,c('children','charges')]

plot(x = input$children, y = input$charges,
     xlab = "Children",
     ylab = "Charges",
     xlim = c(0, 6),
     ylim = c(1000,60000),
     main = "Dependent vs independent variable"
)
```

We can see that there is very weak negative relationship between the children and the charges.
Based on the above scatterplot we can say that people with no children have higher medical costs, it means there are other factors that affecting the charges. 


## Step 3
Pick up one pair of "dependent variable vs independent variable"
- Construct the linear regression model.
- Draw the scatter plot "with line"
- Write down your observations


Linear Regression is used predict or estimate the value of a dependent variable by modelling it against one or more independent variables.
We will look into our liner regression model for the age vs charges with the function and the scatterplot with a line. 



```{r}
#simple linear regression
data <- lm(insurance_2$charges ~ insurance_2$age)
summary(data)

#The error rate can be estimated by dividing the RSE by the mean outcome variable
sigma(data)/mean(insurance_2$charges)


```

F-statistic - Used to estimate the significance of a regression model as a whole. The higher the parameter value, the better it is. Our value is 133.2, which is good.

t value - A criteria based on the Student's t distribution. The value of the parameter in linear regression indicates the significance of the factor; it is generally accepted that at t> 2 the factor is significant for the model.

p value - This is the probability that the hypothesis that the independent variables do not explain the dynamics of the dependent variable is true zero. If the p value is below the threshold (.05 or .01 for the most demanding), then the null hypothesis is false. The lower the better:-)

In our plot the p-value is 2.2e-16, which is 2.2 X 10 ^ -16
If we test with 95% confidence or 5% significance, our P-value of 2.2 X 10 ^-16 is very less compared to 5%=0.05. So we will reject the null hypothesis

Our prediction equation:
charges = 3100 + 258 * age

```{r}
# we need to convert the smoker variable  from categorical to numerical, because it is affecting our main correlation pair

insurance_2$smoker <- as.factor(insurance_2$smoker)
```



```{r}
#scatterplot with the line
simple_lin <- insurance_2[,c('age','charges')]
plot(x = input$age, y = input$charges, 
     xlab = "Age",
     ylab = "Charges",
     xlim = c(18, 70),
     ylim = c(0,60000),
     main = "Age vs Charges",
     col = "lightblue")

abline(lm(input$charges ~ input$age, data = insurance_2), col="pink", lwd=2)
linearReg <- lm(input$age ~ input$charges, data = insurance_2)
print(linearReg)
```

```{r}
# we are creating the scatterplot in different colors to be able to see why do we have 3 ranges in age vs charges relationship and which other variable affecting it.
ggplot(insurance_2, aes(age,charges, col=smoker)) + geom_point(size=0.5) + scale_y_continuous(breaks = seq(0,65000,5000))
```
Based on the scatterplots above we see that at the bottom layer of our linear relationship we have very nice, smooth increase in charges with age, for people who doesn't smoke (red dots). Then at the middle and upper layer we can say that charges increased a lot for the smokers (green dots).
Because of influence of smoker variable on our strongest correlation pair our regression line is passing slightly above the main set of values. Later on we will see the reason for it. Still the regression line is very useful for forecasting our procedures. It is showing the interrelation of our dependent (y) variable with our independent(x) variable.



Further in multiple regression we will analyze it in more details.



## Step 4

***Multiple Regression**

- Try regression with several variables, if possible.
- Additional: try other regression algorithms.

```{r}
#multiple regression
#compute the model coefficients (all x variables)
multiple_model <- lm(charges ~ age + sex + bmi + children + smoker + region, data = insurance_2)
summary(multiple_model)


#The error rate can be estimated by dividing the RSE by the mean outcome variable
sigma(multiple_model)/mean(insurance_2$charges)

```
The first step in interpreting the multiple regression analysis is to examine the F-statistic and the associated p-value.

In our plot, it can be seen that p-value of the F-statistic is < 2.2e-16 for the Age, which is highly significant. This means that, Age, smoker and bmi. Age variable is one of the predictor variables that significantly related to the outcome variable. Smoker is categorical variable, so we would use it just to show the influence on the age vs charges relationship.

Based on coefficients table we can see that t value evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero. Also we can see that changes in Age and smoker are highly associated to changes in charges while changes in other variables are not significantly associated with charges.

We have a decent R2 of 0.7511, which implies 75% of the variation of charges could be explained by our set of included independent variables.

Because children and region variables are not significant, we can remove it from our model and recreate it without it.

```{r}
#compute the model coefficients
model <- lm(charges ~ age + smoker + bmi, data = insurance_2)
summary(model)

#The error rate can be estimated by dividing the RSE by the mean outcome variable
sigma(model)/mean(insurance_2$charges)
```

To assess model accuracy, we have to take a look at other values like R2 and RSE (residual standard error or sigma)

The RSE estimate gives a measure of error of prediction. The lower the RSE, the more accurate the model.
Corresponding to 45% error rate, it is much better than in the simple model, where RSE was 86% error rate.

In multiple linear regression, the R2 represents the correlation coefficient between the observed values of the outcome variable (y) and the fitted (i.e., predicted) values of y. For this reason, the value of R will always be positive and will range from zero to one. 
R2 represents the proportion of variance, in the outcome variable y, that may be predicted by knowing the value of the x variables. An R2 value close to 1 indicates that the model explains a large portion of the variance in the outcome variable. In our case it is 0.74
The adjustment in the “Adjusted R Square” value in the summary output is a correction for the number of x variables included in the prediction model.
In our plot, with age and bmi predicator variables, the adjusted R2 = 0,74, meaning that "74% of the variance in the measure of charges can be predicted by age, smoker and bmi. In comparison, this model is much better than our simple liner regression model with only age, which had an adjusted R2 of 0.09.

Now we can try recreate our scatterplot with the regression line and see how it will pass when we include our variables with the highest correlation.
 
```{r}

#compute and visualize the scatterplot with the regression line (charges, age, smoker)

model3 <- lm(charges ~ age+smoker, insurance_2)
summary(model3)

intercepts <- c(coef(model3)["(Intercept)"], coef(model3)["(Intercept)"] + 
coef(model3)["smokeryes"])
lines.df <- data.frame(intercepts = intercepts,
slopes  = rep(coef(model3)["age"], 2),
smoker = levels(insurance_2$smoker))
qplot(x = age, y = charges, color = smoker, data = insurance_2) + 
geom_abline(aes(intercept = intercepts, slope = slopes, color =smoker),
data = lines.df) + scale_y_continuous(breaks = seq(0,65000,5000))

#The error rate can be estimated by dividing the RSE by the mean outcome variable
sigma(model3)/mean(insurance_2$charges)
```

Based on our scatterplot above we can see that we 2 parallel lines, which means we have two regression equations having the same slope, but different intercepts. Slope of the regression line we can see in column with Estimate values for the variable age -  274.55. Intercept for the smoker is much higher than for non smoker, 23627.66. Meaning of it is that medical charges for the smoker is higher by $23k (approx) than for the person who doesn't smoke. Of course the age will be considered as well, but the charges are very much depend on if the person smokes or not.


To summarize all of the above.
The RSE value lowest in the model with all the variables 45%, the next model with slighlty higher RSE is the model with charges/age/smoker/bmi and with a little bit higher RSE 48% is the model with charges/age/smoker. Other variables like sex and region are very insignificant in affecting the outcome of the charges variable.
So based on all the analyses above we can conclude that multiple regression model with charges/age/smoker/bmi provides the most valuable evaluation parameters.





## Stage 4
Step 1

- Pick up a lottery. For example, https://www.eurojackpot.com/ etc. Do not pick up
bingo type lotteries.
- Describe the lottery.
- Describe the winning condition
Step 2
Compute the probabilities of winning for each case. For example, as in here
https://en.wikipedia.org/wiki/Eurojackpot

Step 3
Explain your computation.

Step 4
Write a small report on that.

I am not familiar with lottery games.
So I searched for the most popular lotteries in the world (https://www.wonderslist.com/top-10-best-lottery-games-world/)
I decided to go with US Powerball.

Based on the rules, you must select five numbers from a pool between 1 and 69 and one Powerball between 1 and 26. The Powerball you select can be the same as one of the five main numbers. If you match all five numbers and the Powerball, you win the top prize. By guessing just the main numbers and not the Powerball, you can qualify for the $1 million second prize. 
The order of the balls is not important, so for my calculations I will use combinations and not permutations.

***C(69,5) = 69!/(5!(69-5)!) = 11238513 So, probability of the wining the prize if you guess 5 numbers is 1/11238513 ≈= 0.000000089
And there are 26 ways to select the Powerball, so resulting in 26 x 11238513 = 292201338 possible selections. The probability to win the main prize, by guessing all 6 numbers is 1/292201338

We  can calculate the rest of the possibilities

***Guessing all five balls but not the Powerball:

25/292201338 ≈= 0.0000000856

***Guessing four of the five balls and the Powerball:

C(5,4) = 5 ways to guess four of the five. The 5th ball is one of the remaining 64 that were not drawn, so C(64,1) = 64 ways for this to happen. There is only 1 way to guess Powerball.
5 x 64 x 1 = 320 ways to guess four of the five balls and Powerball. 
Probability of this to happen is 320/292201338 ≈= 0.0000010951

***Guessing four of the five balls but not the Powerball:

C(5,4) = 5 ways to guess four of the five. The 5th ball is one of the remaining 64 that were not drawn, so C(64,1) = 64 ways for this to happen. This time, there 25 ways to not guess the Powerball.
5 x 64 x 25 = 8000 ways to guess four of the five balls and not the Powerball. 
Probability of this to happen is 8000/292201338 ≈= 0.0000273784

***Guessing three of the five balls and the Powerball:

C(5,3) = 10 ways to guess three of the five. The remaining balls are the ones of the remaining 64 that were not drawn, so C(64,2) = 2016 ways for this to happen. There is only one way to guess Powerball.
10 x 2016 x 1 = 20160 ways to guess three of the five balls and Powerball. 
Probability of this to happen is 20160/292201338 ≈= 0.0000689935

***Guessing three of the five balls but not the Powerball:

C(5,3) = 10 ways to guess three of the five. The remaining balls are the ones of the remaining 64 that were not drawn, so C(64,2) = 2016 ways for this to happen. This time, there 25 ways to not guess the Powerball.
10 x 2016 x 25 = 504000 ways to guess three of the five balls and not the Powerball. 
Probability of this to happen is 504000/292201338 ≈= 0.0017248381

***Guessing two of the five balls and the Powerball:

C(5,2) = 10 ways to guess two of the five. The remaining balls are the ones of the remaining 64 that were not drawn, so C(64,3) = 41664 ways for this to happen. There is only one way to guess Powerball.
10 x 41664 x 1 = 416640 ways to guess two of the five balls and Powerball. 
Probability of this to happen is 416640/292201338 ≈= 0.0014258662

***Guessing one of the five balls and the Powerball:

C(5,4) = 5 ways to guess one of the five. The remaining balls are the ones of the remaining 64 that were not drawn, so C(64,4) = 635376 ways for this to happen. There is only one way to guess Powerball.
5 x 635376 x 1 = 3176880 ways to guess one of the five balls and Powerball. 
Probability of this to happen is 3176880/292201338 ≈= 0.0108722295

Guessing just the Powerball, but none of the other balls:

It is possible to win if non of five balls guessed, but only Powerball guessed. So C(64,5) = 7624512 ways for this to happen. There is only one way to guess Powerball. Meaning that we have 7624512 ways not to guess any balls except for the Powerball.
Probability of this to happen is 7624512/292201338 ≈= 0.0260933507

After calculating the above probabilities of the winning in the US Powerball lottery, I think everyone can see that the chance to win the main price is so small that it is close to impossible. Very nice exercise! 


## Stage 5

#### Step 1
- Split your dataset into two sets: training and testing sets.
- This splitting must be done randomly (if you don't have timeseries as part of the set,
when the process is more complicated)
- The proportions can be different, for example 80/20, 70/30, 90/10, where the bigger
part is the training set, and the smaller part - testing set.


When we built our linear regression model based on full dataset it is hard to predict how it will perform with new data.
So we will split our dataset into a 80/20 sample (training/
test). 
Later, by calculating accuracy measures (like min_max accuracy) and error rates (MAPE or MSE), we will see the prediction accuracy of the model.

```{r}
#creating the training and testing data
# setting seed to reproduce results of random sampling
set.seed(100)
trainingRowIndex <- sample(1:nrow(insurance_2), 0.8*nrow(insurance_2))
train <- insurance_2[trainingRowIndex, ] 
test <- insurance_2[-trainingRowIndex, ]    

#Training Data
str(train)
summary(train)

```
```{r}
#Test date
str(test)
summary(test)
```

We can see that our Training set contains 1063 observations
and Test set has 266 observations.
Also with summary function we can see the values for the particular column. We can easily compare them between the training and the testing sets.



#### Step 2
- Train the regression model on training set.
- You can use different sets of features (if possible), with one or more variables.
- You can train different types of the models (if you wish to try more), but at least you need to try linear regression.



```{r}
#visualizing our first model
ggplot(data= insurance_2)+geom_point(aes(x=age,y=charges,color=smoker)) 
ggplot(data= insurance_2)+geom_point(aes(x=smoker,y=charges,color=smoker)) 
ggplot(data= insurance_2)+geom_point(aes(x=bmi,y=charges,color=smoker))
ggplot(data= insurance_2)+geom_point(aes(x=children,y=charges,color=smoker)) 
ggplot(data= insurance_2)+geom_point(aes(x=region,y=charges,color=smoker))
ggplot(data= insurance_2)+geom_point(aes(x=sex,y=charges,color=smoker))


```




```{r}
#build the model on the training set 
#computing and visualizing the training set 
model5 <- lm(charges ~ age + smoker +bmi + region + children + sex, data = train)
#chargesPred <- predict(model5, test)
summary(model5)

```


```{r}
# residuals vs fitted values helps us to detect non-linearity, unequal error variances, and outliers.
plot(model5, which = 1, col = "blue")
```

We can see that fitted values increase and linearity seems to hold reasonably well, as the red line is close to the dashed line, with slightly curved pattern. We can also note heteroskedasticity. Finally, points 578, 1301, and 243 may be outliers, with large residual values.


 #### Step 3
- Test you model on testing set.
- Evaluate how "well" your model performs.
- For that you can apply different metrics (MAPE, RMSE, MAE, etc.)
- If it is possible, change the type of the model or set of features, and compare the
results.
- Write report.


Now we can calculate prediction accuracy and error rate. A higher correlation accuracy implies that the actuals and predicted values have similar directional movement.

```{r}
#predict the  first model
pred1 <- predict(model5, test)
actuals_preds1 <- data.frame(cbind(actuals = test$charges, predicted = pred1))
cor(actuals_preds1)
```

MinMaxAccuracy = mean(min(actuals, predicteds)/max(actuals, predicteds)
```{r}
#Min-Max Calculations
min_max_accuracy <- mean(apply(actuals_preds1, 1, min) / apply(actuals_preds1, 1, max))  
min_max_accuracy
```

MeanAbsolutePercentageError(MAPE) = mean(abc(predicteds?actuals)/actuals)
```{r}
#MAPE Calculations
mape <- mean(abs((actuals_preds1$predicted - actuals_preds1$actuals))/actuals_preds1$actuals)  
mape
```

We can compute all other error metrics with DMwR function
```{r}
DMwR::regr.eval(actuals_preds1$actuals, actuals_preds1$predicted)
```
The Mean Absolute Error (MAE) is a very good to measure forecast accuracy, it represents the difference between the original and predicted values extracted by squared the average difference over the data set.
The Root Mean Squared Error (RMSE) is defined as the square root of the average squared error.
MAPE is the sum of the individual absolute errors divided by the demand (each period separately). It is the average of the percentage errors.





***Model 2***
To compare model 1 with model 2 we have made certain improvements to the data and will test how it will perform.
```{r}
#improvements
# convertion to numeric values
insurance_new <- insurance_2
insurance_new$smoker = as.factor(insurance_new$smoker)
insurance_new$smoker = as.numeric(insurance_new$smoker)
insurance_new$children = as.factor(insurance_new$children)
insurance_new$children = as.numeric(insurance_new$children)
insurance_new$region = as.factor(insurance_new$region)
insurance_new$region = as.numeric(insurance_new$region)
insurance_new$bmi30 <- ifelse(insurance_new$bmi >= 30, 1, 0)
```

```{r}
# Create Training and Test data -- model2 (after the improvements)
trainingRowIndex <- sample(1:nrow(insurance_new), 0.8 * nrow(insurance_new))
train2 <- insurance_new[trainingRowIndex,]
test2 <- insurance_new[-trainingRowIndex,] 


#view
str(train2)
summary(train2)

```
```{r}
#view test data for second model
str(test2)
summary(test2)
```

```{r}
# Build the model on training data
model6 <- lm(charges ~ age + smoker + bmi30 + region + children + sex, data = train2)
chargesPred <- predict(model6, test2)
summary(model6)

```


```{r}
# residuals vs fitted values helps us to detect non-linearity, unequal error variances, and outliers.
plot(model6, which = 1, col = "blue")
```
We can see that fitted values increase and linearity seems to hold reasonably well, as the red line is close to the dashed line, with slightly curved pattern. We can also note heteroskedasticity in our plot. The points 578, 220, and 243 may be outliers, with large residual values.

```{r}
#visualizing our model 2
#color smoker, 2 means  = yes, 1 means  = no
ggplot(data= insurance_new)+geom_point(aes(x=age,y=charges,color=smoker)) 
ggplot(data= insurance_new)+geom_point(aes(x=smoker,y=charges,color=smoker)) 
ggplot(data= insurance_new)+geom_point(aes(x=bmi,y=charges,color=smoker))
ggplot(data= insurance_new)+geom_point(aes(x=children,y=charges,color=smoker)) 
ggplot(data= insurance_new)+geom_point(aes(x=region,y=charges,color=smoker))
ggplot(data= insurance_new)+geom_point(aes(x=sex,y=charges,color=smoker))



```


#### Testing

```{r}
# predict
pred2 <- predict(model6, test2)
actuals_preds2 <- data.frame(cbind(actuals = test2$charges, predicted = pred2))
cor(actuals_preds2)
```


MinMaxAccuracy = mean(min(actuals, predicteds)/max(actuals, predicteds)

```{r}
#Min-Max Calculations
min_max_accuracy <- mean(apply(actuals_preds2, 1, min) / apply(actuals_preds2, 1, max))  
min_max_accuracy
``` 
MeanAbsolutePercentageError(MAPE) = mean(abc(predicteds?actuals)/actuals) 

```{r}
#MAPE Calculations
mape <- mean(abs((actuals_preds2$predicted - actuals_preds2$actuals))/actuals_preds2$actuals)  
mape

```



```{r}
#other metrics, RMSE, MAE
DMwR::regr.eval(actuals_preds2$actuals, actuals_preds2$predicted)
```


When we compare 2 models, one with original training data and one with improvements, we can see that prediction power increased after the improvements.
Multiple R-squared value in the first model (model5) is 0.7465 and the second model (model6) is 0.7688 , so it is pretty good improvement.
In our prediction matrix we can see that in the first model it was 0.8768414 (approx 88%) which is much higher than in the second model (model6), where the value is 0.8261449 (approx 84%).
F-statistic that used to estimate the significance of a regression model as a whole in our second model (model6) is much higher 585.2 than in the first model (model5) - 257.7
Min_Max Accuracy with the value closer to 1, means the better prediction, if measure is 1 it will be perfect model. In our sets we can see that in the second model the value is 0.712 and in the first model is 0.708, so the second model provides better prediction.
MAPE express accuracy as a percentage of the error and in our models it is the same 4%.
MAE  measures the average magnitude of the errors in a set of predictions, in our models the lowest value we have in the first model (model5) - 3652, the smaller MAE suggest that the model is better at prediction.
RMSE showing us that on average, our predictions varied from observed values by an absolute measure 6122.22 in the second model and 5264 in the first model.

